{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc58741f-cee8-4a60-84a5-143c81f2745b",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "**Min-max scaling** (usually called feature scaling) performs a linear transformation on the original data. This technique gets all the scaled data in the range (0, 1). The formula to achieve this is the following:\n",
    "\n",
    "x'=(x-xmin)/(xmax-xmin)\n",
    "\n",
    "Min-max normalization preserves the relationships among the original data values. The cost of having this bounded range is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\\\n",
    "In the example shown below we can see that we have scaled down distance, fare and tip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dc029f4-e477-465a-8f61-887f616cae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
      "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
      "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
      "2 2019-03-27 17:53:01 2019-03-27 18:00:25           1      1.37   7.5  2.36   \n",
      "3 2019-03-10 01:23:59 2019-03-10 01:49:51           1      7.70  27.0  6.15   \n",
      "4 2019-03-30 13:27:42 2019-03-30 13:37:14           3      2.16   9.0  1.10   \n",
      "\n",
      "   tolls  total   color      payment            pickup_zone  \\\n",
      "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
      "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
      "2    0.0  14.16  yellow  credit card          Alphabet City   \n",
      "3    0.0  36.95  yellow  credit card              Hudson Sq   \n",
      "4    0.0  13.40  yellow  credit card           Midtown East   \n",
      "\n",
      "            dropoff_zone pickup_borough dropoff_borough  \n",
      "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
      "1  Upper West Side South      Manhattan       Manhattan  \n",
      "2           West Village      Manhattan       Manhattan  \n",
      "3         Yorkville West      Manhattan       Manhattan  \n",
      "4         Yorkville West      Manhattan       Manhattan  \n",
      "   distance      fare       tip\n",
      "0  0.043597  0.040268  0.064759\n",
      "1  0.021526  0.026846  0.000000\n",
      "2  0.037330  0.043624  0.071084\n",
      "3  0.209809  0.174497  0.185241\n",
      "4  0.058856  0.053691  0.033133\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "df=sns.load_dataset('taxis')\n",
    "a=df.head()\n",
    "print(a)\n",
    "df1=pd.DataFrame(min_max.fit_transform(df[['distance','fare','tip']]),columns=['distance','fare','tip'])\n",
    "b=df1.head()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535fd0ca-c685-4b48-bae5-756008b08486",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "In **unit vector technique** Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector\n",
    "\n",
    "**MinMax normalization technique** re-scales a feature or observation value with distribution value between 0 and 1 or a given range,MinMax shrinks the data within the range of -1 to 1 if there are negative values, and can set the range like [0,1] or [0,5] or [-1,1],This technique responds well if the standard deviation is small and when a distribution is not Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "148ff6d2-a94c-4d37-b8fb-70c8646a5e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n",
    "df=sns.load_dataset('iris')\n",
    "normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "pd.DataFrame(normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]),\n",
    "             columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7c56e-af57-4ecf-9f54-d807461e666a",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "**Principal Component** Analysis is an unsupervised learning algorithm that is used for the dimensionality reduction in machine learning. It is a statistical process that converts the observations of correlated features into a set of linearly uncorrelated features with the help of orthogonal transformation. These new transformed features are called the Principal Components. It is one of the popular tools that is used for exploratory data analysis and predictive modeling. It is a technique to draw strong patterns from the given dataset by reducing the variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee4bece-c652-41e1-8d91-fade9aae3877",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**Data compression**: PCA can be used to reduce the dimensionality of high-dimensional datasets, making them easier to store and analyze. **Feature extraction**: PCA can be used to identify the most important features in a dataset, which can be used to build predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d2902-19c1-4e1d-8075-2b6295488d92",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2c09a89-5ca5-41c0-89c0-e3224a743d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>restaurant_name</th>\n",
       "      <th>cuisine_type</th>\n",
       "      <th>cost_of_the_order</th>\n",
       "      <th>day_of_the_week</th>\n",
       "      <th>rating</th>\n",
       "      <th>food_preparation_time</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1477147</td>\n",
       "      <td>337525</td>\n",
       "      <td>Hangawi</td>\n",
       "      <td>Korean</td>\n",
       "      <td>30.75</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>4.344234</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1477685</td>\n",
       "      <td>358141</td>\n",
       "      <td>Blue Ribbon Sushi Izakaya</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>12.08</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>4.344234</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1477070</td>\n",
       "      <td>66393</td>\n",
       "      <td>Cafe Habana</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>12.23</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1477334</td>\n",
       "      <td>106968</td>\n",
       "      <td>Blue Ribbon Fried Chicken</td>\n",
       "      <td>American</td>\n",
       "      <td>29.20</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1478249</td>\n",
       "      <td>76942</td>\n",
       "      <td>Dirty Bird to Go</td>\n",
       "      <td>American</td>\n",
       "      <td>11.59</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id  customer_id            restaurant_name cuisine_type  \\\n",
       "0   1477147       337525                    Hangawi       Korean   \n",
       "1   1477685       358141  Blue Ribbon Sushi Izakaya     Japanese   \n",
       "2   1477070        66393                Cafe Habana      Mexican   \n",
       "3   1477334       106968  Blue Ribbon Fried Chicken     American   \n",
       "4   1478249        76942           Dirty Bird to Go     American   \n",
       "\n",
       "   cost_of_the_order day_of_the_week    rating  food_preparation_time  \\\n",
       "0              30.75         Weekend  4.344234                     25   \n",
       "1              12.08         Weekend  4.344234                     25   \n",
       "2              12.23         Weekday  5.000000                     23   \n",
       "3              29.20         Weekend  3.000000                     25   \n",
       "4              11.59         Weekday  4.000000                     25   \n",
       "\n",
       "   delivery_time  \n",
       "0             20  \n",
       "1             23  \n",
       "2             28  \n",
       "3             15  \n",
       "4             24  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "df=pd.read_csv(\"food_order.csv\")\n",
    "df1=df\n",
    "#df1.drop(df1[df1['rating']=='Not given'].index,inplace = True)\n",
    "#df1['rating']=df1['rating'].astype(int)\n",
    "#df1.describe()\n",
    "df1.replace(to_replace=\"Not given\",value=\"4.344234\",inplace=True)\n",
    "df1['rating']=df1['rating'].astype(float)\n",
    "df1.describe()\n",
    "#df1=pd.DataFrame(min_max.fit_transform(df[['cost_of_the_order','rating','delivery_time']]),columns=['cost_of_the_order','rating','delivery_time'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c08325d6-96de-4f1e-b9ba-a31f1ee09dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost_of_the_order</th>\n",
       "      <th>rating</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849386</td>\n",
       "      <td>0.672117</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.245960</td>\n",
       "      <td>0.672117</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.799289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.230123</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cost_of_the_order    rating  delivery_time\n",
       "0           0.849386  0.672117       0.277778\n",
       "1           0.245960  0.672117       0.444444\n",
       "2           0.250808  1.000000       0.722222\n",
       "3           0.799289  0.000000       0.000000\n",
       "4           0.230123  0.500000       0.500000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.DataFrame(min_max.fit_transform(df[['cost_of_the_order','rating','delivery_time']]),columns=['cost_of_the_order','rating','delivery_time'])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3285084-db2e-4cf3-8b9b-c43ad2368e8b",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "When working with a dataset that has a high number of features, such as in the case of predicting stock prices,\n",
    "dimensionality reduction techniques like Principal Component Analysis (PCA) can be useful.\n",
    "PCA is a statistical technique that aims to reduce the dimensionality of a dataset while preserving the most important information.\n",
    "\n",
    "Here's an explanation of how PCA can be used to reduce the dimensionality of a stock price prediction dataset\n",
    "Data Preprocessing: Begin by preprocessing the dataset, which involves handling missing values,\n",
    "scaling the features, and encoding categorical variables if any.\n",
    "\n",
    "1. **Feature Standardization**: Perform feature standardization on the dataset. PCA works best when the features are on a similar scale, so it is crucial to standardize the features by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized dataset. The covariance matrix provides information about the relationships between pairs of features and is required for performing PCA.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. His step finds the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in the original feature space, while the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Select Principal Components**: Sort the eigenvalues in descending order and select the top k eigenvectors (principal components) that correspond to the highest eigenvalues. The number of principal components selected determines the reduced dimensionality of the dataset.\n",
    "\n",
    "5. **Project Data**: Project the standardized dataset onto the selected principal components to obtain the reduced-dimensional representation of the data. This is achieved by taking the dot product of the standardized dataset and the eigenvectors corresponding to the selected principal components.\n",
    "\n",
    "6. **Explained Variance Ratio**: Compute the explained variance ratio to understand how much of the total variance is explained by each principal component. This information helps in assessing the amount of information retained after dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f4006-402c-492f-905c-eb5c22f491e6",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36521040-6137-4cc8-80ec-52f082a0d2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -1.000000\n",
       "1 -0.578947\n",
       "2 -0.052632\n",
       "3  0.473684\n",
       "4  1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler(feature_range=(-1,1))\n",
    "df=pd.DataFrame([1, 5, 10, 15, 20])\n",
    "\n",
    "df1=pd.DataFrame(min_max.fit_transform(df))\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d494d-e23b-4557-8b94-33e7e1a8fe81",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To determine the number of principal components to retain when performing PCA, several factors should be considered,\n",
    "including the desired level of information retention, the trade-off between dimensionality reduction and predictive performance, and the specific characteristics of the dataset.\n",
    "\n",
    "1. **Compute the Covariance Matrix**: Calculate the covariance matrix of the dataset, which measures the relationships between pairs of features. This matrix provides insights into the amount of correlation or redundancy present in the data.\n",
    "\n",
    "2. **Eigenvalue Analysis**: Perform eigenvalue analysis on the covariance matrix to understand the variance explained by each principal component. The eigenvalues represent the amount of variance explained by each corresponding eigenvector (principal component).\n",
    "\n",
    "3. **Explained Variance Ratio**: Compute the explained variance ratio, which is the proportion of the total variance explained by each principal component. The explained variance ratio is obtained by dividing each eigenvalue by the sum of all eigenvalues.\n",
    "\n",
    "4. **Scree Plot**: Plot the explained variance ratio against the number of principal components. This scree plot visually represents the cumulative variance explained as the number of principal components increases.\n",
    "\n",
    "5. **Determine the Threshold**: Decide on a threshold for the amount of variance you want to retain. For example, you may aim to retain 90%, 95%, or 99% of the total variance. This threshold determines the number of principal components to retain.\n",
    "\n",
    "6. **Evaluate Trade-offs**: Consider the trade-off between dimensionality reduction and predictive performance. Retaining more principal components preserves more information but increases the dimensionality of the transformed dataset. This trade-off can impact computational complexity and potential overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
